# TODO

- on Hopper GPU, add/test use_fp8_context_fmha (doesn't work on L20/Ada GPU)
- Speicial topics
  - Long context, Stream LLM
  - LoRA switch models
- Add more performance test methods
